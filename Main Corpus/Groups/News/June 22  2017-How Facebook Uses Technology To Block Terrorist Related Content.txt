 Facebook says it wants to keep from being used by extremists. The company used by billions of people is under pressure not to be a platform for violence. More than once people have committed murder on Facebook. Terrorists have proclaimed allegiance to ISIS on Facebook or won new recruits there. Facebook's Monika Bickert oversees an effort to stop that.  We are focused on real-world harm so harm in the physical world. That means that - you know for things like terrorism recruitment or terrorism propaganda we want to make sure that it is not on the site because we think that that could lead to real-world harm.  It is a vast challenge and delicate. Facebook wants to block dangerous content without blocking free speech. Its techniques include hiring more human monitors of doubtful content. Bickert also hopes to block some offensive images before they are published using image-matching software.  To tell if somebody is for instance trying to upload a known propaganda video.  Blocking harmful or violent videos I guess before they reach anyone. How would that work?  There is software that we have that allows us to recognize if a video that someone's trying to upload to Facebook is a video we have seen before. So let us say that somebody uploads an ISIS formal propaganda video. Somebody reports that or somebody tells us about that. We look at that video. Then we can use this software to create what is called a hash or a digital fingerprint of that video so that if somebody else tries to upload that video in the future we would recognize it even before the video hits the site. And you know I want to point out that it does not necessarily mean that we would take automated action. There are some types of videos that would violate our policies no matter what like a beheading video. But there are other times where we need people to actually review the content that this software is flagging for us.  What if somebody creates a - god forbid - a fresh beheading? Do you have an algorithm or software that can recognize that as it is happening?  Photo-matching software is not going to recognize that. But if we can find out from the community you know what the new image contains - and we also talked to others in industry about this so that whoever finds it first can share it with the others - then we can go ahead and create a digital hash of that and stop anybody from uploading it to Facebook.  Since you have a background in law enforcement I know you are familiar with the phrase prior restraint which is something that the U.S. government is never supposed to do when it comes to free speech. There may be speech that is - can be punished in some way but there should not be prior restraint of publication. Here you are a private company and you are contemplating prior restraint. What are the specific instances when you think it is OK for you to do that?  Well I mean first I want to point out that as a social media company we set the terms and let our people know what the terms are for when they come to Facebook. So we have made it really clear for a long time that we do not allow terror propaganda. And we are going to do everything we can to stop it from hitting the site as early as possible. We are really looking at the context of how something was shared. If it is terrorism propaganda we are going to remove it. If somebody is sharing it for news value or to condemn violence we may leave it up.  What if there is talk of a beheading?  Our policy is that we will remove anything that is promoting or glorifying these terror groups or their actions. So you can imagine people discussing a terror attack. If they are saying this is really wonderful this is funny I love what happened to these people and those posts are reported to us we would remove those for celebrating terrorism.  What if it is someone who is denouncing the United States or denouncing the West in ways that someone might find to be unfair or building up hatred against the United States?  We do allow political speech. So you know people - some people are going to express dissatisfaction or even hatred for countries or for their foreign policies and that is something that you can do on Facebook. Where we draw the line is when it comes to promoting or celebrating violence.  Let me ask about information bubbles. People in the United States have been more and more forcefully reminded that many of us risk being in an information bubble where the more content of a certain slant that we click on the more content like that we get. And there are left-wing information bubbles and right-wing information bubbles. Are there terrorist information bubbles do you think?  Well there is maybe two different things we are talking about here. And one is whether it is hate speech or terror propaganda that is being discussed among a certain group of people. And another is whether or not there is speech that pushes back on that. We do find that there is a lot of speech that pushes back on ideas like promoting terror groups or hate speech. And in fact we are involved in trying to enable and promote some of those campaigns.  Oh but that is kind of my question because as everybody knows you can have lots of speech that pushes back on Democrats that Democrats never see and lots of speech that pushes back on Republicans that Republicans never see. Are there information bubbles for people who are extremists or leaning toward extremism where they might spend all day on Facebook and never see anything that challenges their views?  There is certainly - you know there is certainly always an opportunity for people to engage in groups they want to engage in. That is one of the reasons that we are focused on technology to help us find terror propaganda no matter where it is.  But do your algorithms which move automatically allow people - even if they are not seeing a beheading video - even if you block that you might - you might have people who see nothing all day but borderline-extremist content the kind of thing that might encourage them to become more violent?  That is one of the reasons that we are focused on not only using our technology but also building our partnerships so that we can help people push back on that kind of messaging.  Does your strategy include invading or popping some of those bubbles dropping in some anti-extremist content to people who seem to be tending in that direction?  We are not content creators. What we try to do is help the people who are creating good content against extremism against hatred make their content succeed on Facebook. So you know we have done...  So does that mean you tweak your algorithms so they would get to the right people?  No no we do not. But what we do is we study - and we have commissioned research a couple different times and published that research. We study the best ways for speech against terrorism and extremism to flourish. And then we try to help these civil society groups and others use those techniques to make their speech succeed and reach a lot of people on Facebook.  Monika Bickert thanks very much.  Thank you very much.  She is Facebook's director of global policy management. 