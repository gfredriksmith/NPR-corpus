 When a person listens to Tchaikovsky's Fifth Symphony it sounds like this.   But to a monkey that symphony may sound more like this.   That is one finding from a study that compared the way human and monkey brains process sounds. NPR's Jon Hamilton reports the research may help explain why people are able to appreciate music and understand speech.  The study began with a bet between two scientists. One of them is Bevil Conway a researcher at the National Institutes of Health. And one day Conway was talking to a graduate student who would done research showing that the human brain is very attuned to a sound's pitch.  I was like well if you see that and it is a robust finding you see in humans we will see it in monkeys.  But Sam Norman-Haignere who is now a postdoc at Columbia University thought monkey brains might be different.  Honestly I was not sure. I mean that is usually a sign of a good experiment you know when you do not know what the outcome is.  So the two scientists and some colleagues did that experiment. They used a special type of MRI to monitor the brains of six people and five macaque monkeys as they listened to a range of sounds. Some of the sounds were more like music where changes in pitch are obvious.   Some were more like noise.   And Conway says it did not take long to realize he would lost his bet.  In humans you see this beautiful organization - pitch bias. And it is clear as day in a single scan. And the drum-roll question was what do we see in monkeys? And there we see nothing.  The study which appears in the journal Nature Neuroscience found that monkey brains just were not that sensitive to pitch. And Conway says that surprised him because his research had shown that the two species are nearly identical when it comes to processing visual information.  When I look at something I am pretty sure that the monkey is seeing the same thing that I am seeing. But here in the auditory domain it seems fundamentally different.  But why? Norman-Haignere says one answer may be our exposure to speech and music.  Both speech and music are highly complex structured sounds. And it is totally plausible that the brain has developed regions that are highly tuned to those structures.  Norman-Haignere says that tuning could be something we are born with.  It is plausible that there is something in our genetic code that causes those regions to develop the way they are and to be located where they are.  Or it could be that these brain regions develop as children listen to music and speech. Conway says regardless subtle changes in pitch and tone seem to be critical when people want to convey emotion.  You can know whether or not I am angry or sad or questioning or confused. And you can get almost all of that meaning just from the tone.  So Conway says it makes sense that music can also convey emotion even when it does not include any words. Jon Hamilton NPR News. 