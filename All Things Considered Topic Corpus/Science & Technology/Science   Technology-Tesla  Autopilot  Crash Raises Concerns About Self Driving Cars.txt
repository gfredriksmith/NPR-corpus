 A fatal crash with a Tesla in autopilot mode is raising questions about self-driving car technology. We learned about the crash yesterday but it happened back in May on a highway in Florida. The car was on a divided highway when a tractor trailer turned across the lanes up ahead. The car's autopilot system and the driver apparently failed to see the vehicle. According to Tesla the brake was never applied before the crash. Alex Davies is with Wired magazine and he is been writing about this. Welcome to the program Alex.  Thanks for having me.  And please describe this autopilot feature to us. How does it work?  Sure. So Tesla's autopilot is not actually all that different from an advanced cruise control you can buy in a new Mercedes or Audi or even cheaper cars. When you are in the car and you are on the highway as long as you are going at least 18 miles an hour you hit a little button on one of the stalks coming off the steering wheel. And from that point on the car uses cameras in the front to pick up lane lines and it uses radars to look for other cars. And its basic mission is to keep you in the middle of your lane and a safe distance from other cars. So if the semi in front of it slows down to 55 miles an hour it will also slow down.  Well based on what we have heard about this crash it sounds like there is a blind spot in its capabilities.  There certainly seems to have been some sort of glitch. Now Tesla is saying that when the tractor trailer turned in front of this Tesla because the trailer was all white that the car did not see it against a similarly white sky. That makes sense or that could make sense for the cameras but it is harder to understand why the radar would not pick up the tractor trailer because the radar does not see color. It sees objects. Tesla has not given a really clear explanation of that but they are quick to say hey this is a beta system. You know it is still technically in testing even though we have given it out to the public.  I should add by the way that there are a great many white vehicles on the road so that alone is not a small problem. This is one fatal car crash. It is estimated there were 35000 motor vehicle deaths last year. But because it was a self-driving car at least a car in auto pilot it raises some very interesting questions. I mean does Tesla say we do not guarantee that this car will miss a car turning in front of it it is up to you you would better be sure to back-up the machine at all times?  Oh they definitely think that. When you get a car with autopilot autopilot is automatically turned off. You have to turn it on and in doing so effectively sign a box that says hey I know this is a beta feature. I know that I the driver am ultimately responsible for the behavior of the car. I am not going to take my eyes off the road. I am not going to take my hands off the wheel. And this is purely a convenience feature and a safety aide.  Is there a possibility though here of a gap between what drivers formally signed onto and say they understand and what they are actually expecting the technology to do for them when they are at the wheel?  Oh absolutely. And I think you saw that within a week of this system being released which was in mid-October of last year. There is a whole new genre of YouTube videos of Tesla drivers playing around with autopilot. There is one video of a guy filming his car driving itself from the backseat. So I think as with anything there is a difference between signing a user agreement and actually reading it let alone obeying the entire thing.  Does that agreement that the driver makes does that absolve Tesla of any of any liability here? I mean is it all user error after that?  You know that is not entirely clear. And I spoke with one legal expert who told me that while you know it is good for Tesla that they did have people sign that agreement that ow the National Highway Traffic Safety Administration which is investigating this accident could look and say well hey Tesla like you knew people were misusing this. You knew there was a serious risk that this was a feature that your customers were not using safely. And if Tesla did not take adequate steps to remedy that then maybe they are not quite as absolved as they would hope.  Can Tesla argue that its cars have prevented more accidents than they have caused?  They certainly can argue that and in fact they are already arguing it. That was the central argument of the blog post they put up yesterday in response to the crash. And I think it is a worthy argument it is just you know it is still hard for safety regulators to say well you prevented crashes. But if they do not deem this safe feature something that owners should have access to then all of that is pulled into question.  And if I have a member of my family who is trusting in an autopilot mode in a Tesla and dies in a car crash it is small solace that five other people did not die in car crashes that week.  Exactly. Small solace especially on the legal side if someone decides to press charges against Tesla.  Alex Davies of Wired magazine thanks for talking with us.  Thank you. 